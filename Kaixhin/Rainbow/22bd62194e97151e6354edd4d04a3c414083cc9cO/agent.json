[{"lineNumber": 13, "col_offset": 2, "nodeName": "__init__", "type": "Callable[[Any, Any, Any], Any]"}, {"lineNumber": 30, "col_offset": 2, "nodeName": "act", "type": "Callable[[Any, Any, Any], Any]"}, {"lineNumber": 36, "col_offset": 2, "nodeName": "learn", "type": "Callable[[Any, Any], Any]"}, {"lineNumber": 60, "col_offset": 2, "nodeName": "update_target_net", "type": "Callable[[Any], Any]"}, {"lineNumber": 63, "col_offset": 2, "nodeName": "save", "type": "Callable[[Any, Any], Any]"}, {"lineNumber": 66, "col_offset": 2, "nodeName": "evaluate_q", "type": "Callable[[Any, Any], Any]"}, {"lineNumber": 69, "col_offset": 2, "nodeName": "train", "type": "Callable[[Any], Any]"}, {"lineNumber": 72, "col_offset": 2, "nodeName": "eval", "type": "Callable[[Any], Any]"}, {"lineNumber": 14, "col_offset": 4, "nodeName": "action_space", "type": "Any"}, {"lineNumber": 15, "col_offset": 4, "nodeName": "batch_size", "type": "Any"}, {"lineNumber": 15, "col_offset": 22, "nodeName": "batch_size", "type": "Any"}, {"lineNumber": 16, "col_offset": 4, "nodeName": "discount", "type": "Any"}, {"lineNumber": 16, "col_offset": 20, "nodeName": "discount", "type": "Any"}, {"lineNumber": 17, "col_offset": 4, "nodeName": "max_gradient_norm", "type": "Any"}, {"lineNumber": 17, "col_offset": 29, "nodeName": "max_gradient_norm", "type": "Any"}, {"lineNumber": 19, "col_offset": 4, "nodeName": "policy_net", "type": "model.DQN"}, {"lineNumber": 24, "col_offset": 4, "nodeName": "target_net", "type": "model.DQN"}, {"lineNumber": 28, "col_offset": 4, "nodeName": "optimiser", "type": "Any"}, {"lineNumber": 37, "col_offset": 4, "nodeName": "transitions", "type": "Any"}, {"lineNumber": 38, "col_offset": 4, "nodeName": "batch", "type": "memory.`namedtuple-Transition-state-action-next_state-reward`"}, {"lineNumber": 40, "col_offset": 4, "nodeName": "states", "type": "Any"}, {"lineNumber": 41, "col_offset": 4, "nodeName": "actions", "type": "Any"}, {"lineNumber": 42, "col_offset": 4, "nodeName": "rewards", "type": "Any"}, {"lineNumber": 43, "col_offset": 4, "nodeName": "non_final_mask", "type": "Any"}, {"lineNumber": 44, "col_offset": 4, "nodeName": "next_states", "type": "Any"}, {"lineNumber": 46, "col_offset": 4, "nodeName": "Qs", "type": "Any"}, {"lineNumber": 47, "col_offset": 4, "nodeName": "next_state_argmax_indices", "type": "Any"}, {"lineNumber": 48, "col_offset": 4, "nodeName": "Qns", "type": "Any"}, {"lineNumber": 50, "col_offset": 4, "nodeName": "volatile", "type": "bool"}, {"lineNumber": 51, "col_offset": 4, "nodeName": "target", "type": "Any"}, {"lineNumber": 53, "col_offset": 4, "nodeName": "loss", "type": "Any"}, {"lineNumber": 14, "col_offset": 4, "nodeName": "self", "type": "Agent"}, {"lineNumber": 14, "col_offset": 24, "nodeName": "action_space", "type": "Any"}, {"lineNumber": 15, "col_offset": 4, "nodeName": "self", "type": "Agent"}, {"lineNumber": 15, "col_offset": 22, "nodeName": "args", "type": "Any"}, {"lineNumber": 16, "col_offset": 4, "nodeName": "self", "type": "Agent"}, {"lineNumber": 16, "col_offset": 20, "nodeName": "args", "type": "Any"}, {"lineNumber": 17, "col_offset": 4, "nodeName": "self", "type": "Agent"}, {"lineNumber": 17, "col_offset": 29, "nodeName": "args", "type": "Any"}, {"lineNumber": 19, "col_offset": 4, "nodeName": "self", "type": "Agent"}, {"lineNumber": 19, "col_offset": 22, "nodeName": "DQN", "type": "Type[model.DQN]"}, {"lineNumber": 19, "col_offset": 26, "nodeName": "args", "type": "Any"}, {"lineNumber": 19, "col_offset": 32, "nodeName": "action_space", "type": "Any"}, {"lineNumber": 20, "col_offset": 7, "nodeName": "model", "type": "Any"}, {"lineNumber": 22, "col_offset": 4, "nodeName": "train", "type": "Any"}, {"lineNumber": 24, "col_offset": 4, "nodeName": "self", "type": "Agent"}, {"lineNumber": 24, "col_offset": 22, "nodeName": "DQN", "type": "Type[model.DQN]"}, {"lineNumber": 24, "col_offset": 26, "nodeName": "args", "type": "Any"}, {"lineNumber": 24, "col_offset": 32, "nodeName": "action_space", "type": "Any"}, {"lineNumber": 25, "col_offset": 4, "nodeName": "update_target_net", "type": "Callable[[], Any]"}, {"lineNumber": 26, "col_offset": 4, "nodeName": "eval", "type": "Any"}, {"lineNumber": 28, "col_offset": 4, "nodeName": "self", "type": "Agent"}, {"lineNumber": 28, "col_offset": 21, "nodeName": "Adam", "type": "Any"}, {"lineNumber": 31, "col_offset": 25, "nodeName": "epsilon", "type": "Any"}, {"lineNumber": 37, "col_offset": 18, "nodeName": "sample", "type": "Any"}, {"lineNumber": 37, "col_offset": 29, "nodeName": "batch_size", "type": "Any"}, {"lineNumber": 38, "col_offset": 12, "nodeName": "Transition", "type": "Type[memory.`namedtuple-Transition-state-action-next_state-reward`]"}, {"lineNumber": 40, "col_offset": 13, "nodeName": "Variable", "type": "Any"}, {"lineNumber": 41, "col_offset": 14, "nodeName": "Variable", "type": "Any"}, {"lineNumber": 42, "col_offset": 14, "nodeName": "Variable", "type": "Any"}, {"lineNumber": 43, "col_offset": 21, "nodeName": "ByteTensor", "type": "Any"}, {"lineNumber": 44, "col_offset": 18, "nodeName": "Variable", "type": "Any"}, {"lineNumber": 46, "col_offset": 9, "nodeName": "gather", "type": "Any"}, {"lineNumber": 46, "col_offset": 43, "nodeName": "actions", "type": "Any"}, {"lineNumber": 48, "col_offset": 10, "nodeName": "Variable", "type": "Any"}, {"lineNumber": 49, "col_offset": 4, "nodeName": "Qns", "type": "Any"}, {"lineNumber": 49, "col_offset": 26, "nodeName": "gather", "type": "Any"}, {"lineNumber": 49, "col_offset": 65, "nodeName": "next_state_argmax_indices", "type": "Any"}, {"lineNumber": 50, "col_offset": 4, "nodeName": "Qns", "type": "Any"}, {"lineNumber": 51, "col_offset": 13, "nodeName": "rewards", "type": "Any"}, {"lineNumber": 53, "col_offset": 11, "nodeName": "smooth_l1_loss", "type": "Any"}, {"lineNumber": 53, "col_offset": 28, "nodeName": "Qs", "type": "Any"}, {"lineNumber": 53, "col_offset": 32, "nodeName": "target", "type": "Any"}, {"lineNumber": 55, "col_offset": 4, "nodeName": "zero_grad", "type": "Any"}, {"lineNumber": 56, "col_offset": 4, "nodeName": "backward", "type": "Any"}, {"lineNumber": 57, "col_offset": 4, "nodeName": "clip_grad_norm", "type": "Any"}, {"lineNumber": 57, "col_offset": 58, "nodeName": "max_gradient_norm", "type": "Any"}, {"lineNumber": 58, "col_offset": 4, "nodeName": "step", "type": "Any"}, {"lineNumber": 61, "col_offset": 4, "nodeName": "load_state_dict", "type": "Any"}, {"lineNumber": 64, "col_offset": 4, "nodeName": "save", "type": "Any"}, {"lineNumber": 67, "col_offset": 11, "nodeName": "data", "type": "Any"}, {"lineNumber": 70, "col_offset": 4, "nodeName": "train", "type": "Any"}, {"lineNumber": 73, "col_offset": 4, "nodeName": "eval", "type": "Any"}, {"lineNumber": 14, "col_offset": 24, "nodeName": "env", "type": "Any"}, {"lineNumber": 19, "col_offset": 32, "nodeName": "self", "type": "Agent"}, {"lineNumber": 20, "col_offset": 7, "nodeName": "args", "type": "Any"}, {"lineNumber": 20, "col_offset": 22, "nodeName": "isfile", "type": "Callable[[Union[str, unicode]], bool]"}, {"lineNumber": 20, "col_offset": 37, "nodeName": "model", "type": "Any"}, {"lineNumber": 21, "col_offset": 6, "nodeName": "load_state_dict", "type": "Any"}, {"lineNumber": 22, "col_offset": 4, "nodeName": "policy_net", "type": "model.DQN"}, {"lineNumber": 24, "col_offset": 32, "nodeName": "self", "type": "Agent"}, {"lineNumber": 25, "col_offset": 4, "nodeName": "self", "type": "Agent"}, {"lineNumber": 26, "col_offset": 4, "nodeName": "target_net", "type": "model.DQN"}, {"lineNumber": 28, "col_offset": 21, "nodeName": "optim", "type": "Any"}, {"lineNumber": 28, "col_offset": 32, "nodeName": "parameters", "type": "Any"}, {"lineNumber": 28, "col_offset": 65, "nodeName": "lr", "type": "Any"}, {"lineNumber": 31, "col_offset": 7, "nodeName": "random", "type": "Callable[[], float]"}, {"lineNumber": 32, "col_offset": 13, "nodeName": "data", "type": "Any"}, {"lineNumber": 34, "col_offset": 13, "nodeName": "randint", "type": "Callable[[int, int], int]"}, {"lineNumber": 37, "col_offset": 18, "nodeName": "mem", "type": "Any"}, {"lineNumber": 37, "col_offset": 29, "nodeName": "self", "type": "Agent"}, {"lineNumber": 40, "col_offset": 22, "nodeName": "stack", "type": "Any"}, {"lineNumber": 40, "col_offset": 34, "nodeName": "state", "type": "Any"}, {"lineNumber": 41, "col_offset": 23, "nodeName": "unsqueeze", "type": "Any"}, {"lineNumber": 42, "col_offset": 23, "nodeName": "Tensor", "type": "Any"}, {"lineNumber": 42, "col_offset": 36, "nodeName": "reward", "type": "Any"}, {"lineNumber": 43, "col_offset": 21, "nodeName": "torch", "type": "Any"}, {"lineNumber": 43, "col_offset": 38, "nodeName": "tuple", "type": "Type[Tuple[Any, ...]]"}, {"lineNumber": 44, "col_offset": 27, "nodeName": "stack", "type": "Any"}, {"lineNumber": 47, "col_offset": 32, "nodeName": "max", "type": "Any"}, {"lineNumber": 48, "col_offset": 19, "nodeName": "zeros", "type": "Any"}, {"lineNumber": 48, "col_offset": 31, "nodeName": "batch_size", "type": "Any"}, {"lineNumber": 49, "col_offset": 8, "nodeName": "non_final_mask", "type": "Any"}, {"lineNumber": 51, "col_offset": 24, "nodeName": "discount", "type": "Any"}, {"lineNumber": 51, "col_offset": 40, "nodeName": "Qns", "type": "Any"}, {"lineNumber": 53, "col_offset": 11, "nodeName": "F", "type": "Any"}, {"lineNumber": 55, "col_offset": 4, "nodeName": "policy_net", "type": "model.DQN"}, {"lineNumber": 56, "col_offset": 4, "nodeName": "loss", "type": "Any"}, {"lineNumber": 57, "col_offset": 4, "nodeName": "utils", "type": "Any"}, {"lineNumber": 57, "col_offset": 28, "nodeName": "parameters", "type": "Any"}, {"lineNumber": 57, "col_offset": 58, "nodeName": "self", "type": "Agent"}, {"lineNumber": 58, "col_offset": 4, "nodeName": "optimiser", "type": "Any"}, {"lineNumber": 61, "col_offset": 4, "nodeName": "target_net", "type": "model.DQN"}, {"lineNumber": 61, "col_offset": 36, "nodeName": "state_dict", "type": "Any"}, {"lineNumber": 64, "col_offset": 4, "nodeName": "torch", "type": "Any"}, {"lineNumber": 64, "col_offset": 15, "nodeName": "state_dict", "type": "Any"}, {"lineNumber": 64, "col_offset": 45, "nodeName": "join", "type": "Callable"}, {"lineNumber": 64, "col_offset": 58, "nodeName": "path", "type": "Any"}, {"lineNumber": 70, "col_offset": 4, "nodeName": "policy_net", "type": "model.DQN"}, {"lineNumber": 73, "col_offset": 4, "nodeName": "policy_net", "type": "model.DQN"}, {"lineNumber": 20, "col_offset": 22, "nodeName": "path", "type": "module"}, {"lineNumber": 20, "col_offset": 37, "nodeName": "args", "type": "Any"}, {"lineNumber": 21, "col_offset": 6, "nodeName": "policy_net", "type": "model.DQN"}, {"lineNumber": 21, "col_offset": 38, "nodeName": "load", "type": "Any"}, {"lineNumber": 21, "col_offset": 49, "nodeName": "model", "type": "Any"}, {"lineNumber": 22, "col_offset": 4, "nodeName": "self", "type": "Agent"}, {"lineNumber": 26, "col_offset": 4, "nodeName": "self", "type": "Agent"}, {"lineNumber": 28, "col_offset": 32, "nodeName": "policy_net", "type": "model.DQN"}, {"lineNumber": 28, "col_offset": 65, "nodeName": "args", "type": "Any"}, {"lineNumber": 31, "col_offset": 7, "nodeName": "random", "type": "module"}, {"lineNumber": 34, "col_offset": 13, "nodeName": "random", "type": "module"}, {"lineNumber": 34, "col_offset": 31, "nodeName": "action_space", "type": "Any"}, {"lineNumber": 38, "col_offset": 24, "nodeName": "zip", "type": "Callable"}, {"lineNumber": 40, "col_offset": 22, "nodeName": "torch", "type": "Any"}, {"lineNumber": 40, "col_offset": 34, "nodeName": "batch", "type": "memory.`namedtuple-Transition-state-action-next_state-reward`"}, {"lineNumber": 42, "col_offset": 23, "nodeName": "torch", "type": "Any"}, {"lineNumber": 42, "col_offset": 36, "nodeName": "batch", "type": "memory.`namedtuple-Transition-state-action-next_state-reward`"}, {"lineNumber": 43, "col_offset": 44, "nodeName": "map", "type": "Callable"}, {"lineNumber": 43, "col_offset": 73, "nodeName": "next_state", "type": "Any"}, {"lineNumber": 44, "col_offset": 27, "nodeName": "torch", "type": "Any"}, {"lineNumber": 44, "col_offset": 39, "nodeName": "tuple", "type": "Type[Tuple[Any, ...]]"}, {"lineNumber": 46, "col_offset": 9, "nodeName": "policy_net", "type": "model.DQN"}, {"lineNumber": 46, "col_offset": 25, "nodeName": "states", "type": "Any"}, {"lineNumber": 48, "col_offset": 19, "nodeName": "torch", "type": "Any"}, {"lineNumber": 48, "col_offset": 31, "nodeName": "self", "type": "Agent"}, {"lineNumber": 49, "col_offset": 26, "nodeName": "target_net", "type": "model.DQN"}, {"lineNumber": 49, "col_offset": 42, "nodeName": "next_states", "type": "Any"}, {"lineNumber": 51, "col_offset": 24, "nodeName": "self", "type": "Agent"}, {"lineNumber": 55, "col_offset": 4, "nodeName": "self", "type": "Agent"}, {"lineNumber": 57, "col_offset": 4, "nodeName": "nn", "type": "Any"}, {"lineNumber": 57, "col_offset": 28, "nodeName": "policy_net", "type": "model.DQN"}, {"lineNumber": 58, "col_offset": 4, "nodeName": "self", "type": "Agent"}, {"lineNumber": 61, "col_offset": 4, "nodeName": "self", "type": "Agent"}, {"lineNumber": 61, "col_offset": 36, "nodeName": "policy_net", "type": "model.DQN"}, {"lineNumber": 64, "col_offset": 15, "nodeName": "policy_net", "type": "model.DQN"}, {"lineNumber": 64, "col_offset": 45, "nodeName": "path", "type": "module"}, {"lineNumber": 70, "col_offset": 4, "nodeName": "self", "type": "Agent"}, {"lineNumber": 73, "col_offset": 4, "nodeName": "self", "type": "Agent"}, {"lineNumber": 20, "col_offset": 22, "nodeName": "os", "type": "module"}, {"lineNumber": 21, "col_offset": 6, "nodeName": "self", "type": "Agent"}, {"lineNumber": 21, "col_offset": 38, "nodeName": "torch", "type": "Any"}, {"lineNumber": 21, "col_offset": 49, "nodeName": "args", "type": "Any"}, {"lineNumber": 28, "col_offset": 32, "nodeName": "self", "type": "Agent"}, {"lineNumber": 34, "col_offset": 31, "nodeName": "self", "type": "Agent"}, {"lineNumber": 38, "col_offset": 29, "nodeName": "transitions", "type": "Any"}, {"lineNumber": 41, "col_offset": 23, "nodeName": "LongTensor", "type": "Any"}, {"lineNumber": 41, "col_offset": 40, "nodeName": "action", "type": "Any"}, {"lineNumber": 43, "col_offset": 73, "nodeName": "batch", "type": "memory.`namedtuple-Transition-state-action-next_state-reward`"}, {"lineNumber": 44, "col_offset": 45, "nodeName": "s", "type": "Any"}, {"lineNumber": 46, "col_offset": 9, "nodeName": "self", "type": "Agent"}, {"lineNumber": 47, "col_offset": 32, "nodeName": "policy_net", "type": "model.DQN"}, {"lineNumber": 47, "col_offset": 48, "nodeName": "next_states", "type": "Any"}, {"lineNumber": 49, "col_offset": 26, "nodeName": "self", "type": "Agent"}, {"lineNumber": 57, "col_offset": 28, "nodeName": "self", "type": "Agent"}, {"lineNumber": 61, "col_offset": 36, "nodeName": "self", "type": "Agent"}, {"lineNumber": 64, "col_offset": 15, "nodeName": "self", "type": "Agent"}, {"lineNumber": 64, "col_offset": 45, "nodeName": "os", "type": "module"}, {"lineNumber": 67, "col_offset": 11, "nodeName": "max", "type": "Any"}, {"lineNumber": 32, "col_offset": 13, "nodeName": "max", "type": "Any"}, {"lineNumber": 41, "col_offset": 23, "nodeName": "torch", "type": "Any"}, {"lineNumber": 41, "col_offset": 40, "nodeName": "batch", "type": "memory.`namedtuple-Transition-state-action-next_state-reward`"}, {"lineNumber": 43, "col_offset": 58, "nodeName": "s", "type": "Any"}, {"lineNumber": 44, "col_offset": 51, "nodeName": "s", "type": "Any"}, {"lineNumber": 44, "col_offset": 56, "nodeName": "next_state", "type": "Any"}, {"lineNumber": 47, "col_offset": 32, "nodeName": "self", "type": "Agent"}, {"lineNumber": 44, "col_offset": 56, "nodeName": "batch", "type": "memory.`namedtuple-Transition-state-action-next_state-reward`"}, {"lineNumber": 44, "col_offset": 76, "nodeName": "s", "type": "Any"}, {"lineNumber": 67, "col_offset": 11, "nodeName": "policy_net", "type": "model.DQN"}, {"lineNumber": 32, "col_offset": 13, "nodeName": "policy_net", "type": "model.DQN"}, {"lineNumber": 67, "col_offset": 11, "nodeName": "self", "type": "Agent"}, {"lineNumber": 67, "col_offset": 27, "nodeName": "unsqueeze", "type": "Any"}, {"lineNumber": 32, "col_offset": 13, "nodeName": "self", "type": "Agent"}, {"lineNumber": 32, "col_offset": 29, "nodeName": "unsqueeze", "type": "Any"}, {"lineNumber": 67, "col_offset": 27, "nodeName": "state", "type": "Any"}, {"lineNumber": 32, "col_offset": 29, "nodeName": "state", "type": "Any"}]
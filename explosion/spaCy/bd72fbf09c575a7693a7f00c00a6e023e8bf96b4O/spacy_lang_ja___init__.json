[{"lineNumber": 10, "col_offset": 24, "nodeName": "object", "type": "Type[object]"}, {"lineNumber": 39, "col_offset": 23, "nodeName": "Defaults", "type": "Any"}, {"lineNumber": 48, "col_offset": 15, "nodeName": "Language", "type": "Any"}, {"lineNumber": 56, "col_offset": 0, "nodeName": "__all__", "type": "List[str]"}, {"lineNumber": 39, "col_offset": 23, "nodeName": "Language", "type": "Any"}, {"lineNumber": 40, "col_offset": 4, "nodeName": "lex_attr_getters", "type": "Dict[Any, Union[Any, Callable[[Any], Any]]]"}, {"lineNumber": 49, "col_offset": 4, "nodeName": "lang", "type": "str"}, {"lineNumber": 50, "col_offset": 4, "nodeName": "Defaults", "type": "Type[JapaneseDefaults]"}, {"lineNumber": 50, "col_offset": 15, "nodeName": "JapaneseDefaults", "type": "Type[JapaneseDefaults]"}, {"lineNumber": 12, "col_offset": 8, "nodeName": "vocab", "type": "Any"}, {"lineNumber": 18, "col_offset": 8, "nodeName": "tokenizer", "type": "Any"}, {"lineNumber": 21, "col_offset": 8, "nodeName": "words", "type": "List[Any]"}, {"lineNumber": 30, "col_offset": 15, "nodeName": "self", "type": "JapaneseTokenizer"}, {"lineNumber": 36, "col_offset": 15, "nodeName": "self", "type": "JapaneseTokenizer"}, {"lineNumber": 40, "col_offset": 23, "nodeName": "dict", "type": "Type[Dict[Any, Any]]"}, {"lineNumber": 40, "col_offset": 28, "nodeName": "lex_attr_getters", "type": "Any"}, {"lineNumber": 41, "col_offset": 4, "nodeName": "lex_attr_getters", "type": "Dict[Any, Union[Any, Callable[[Any], Any]]]"}, {"lineNumber": 12, "col_offset": 8, "nodeName": "self", "type": "JapaneseTokenizer"}, {"lineNumber": 12, "col_offset": 21, "nodeName": "vocab", "type": "Any"}, {"lineNumber": 15, "col_offset": 15, "nodeName": "ImportError", "type": "Type[ImportError]"}, {"lineNumber": 18, "col_offset": 8, "nodeName": "self", "type": "JapaneseTokenizer"}, {"lineNumber": 18, "col_offset": 25, "nodeName": "Tokenizer", "type": "Any"}, {"lineNumber": 21, "col_offset": 17, "nodeName": "surface", "type": "Any"}, {"lineNumber": 22, "col_offset": 15, "nodeName": "Doc", "type": "Any"}, {"lineNumber": 22, "col_offset": 19, "nodeName": "vocab", "type": "Any"}, {"lineNumber": 40, "col_offset": 28, "nodeName": "Defaults", "type": "Any"}, {"lineNumber": 41, "col_offset": 21, "nodeName": "LANG", "type": "Any"}, {"lineNumber": 45, "col_offset": 15, "nodeName": "JapaneseTokenizer", "type": "Type[JapaneseTokenizer]"}, {"lineNumber": 45, "col_offset": 33, "nodeName": "cls", "type": "Type[JapaneseDefaults]"}, {"lineNumber": 45, "col_offset": 38, "nodeName": "nlp", "type": "Any"}, {"lineNumber": 53, "col_offset": 15, "nodeName": "tokenizer", "type": "Any"}, {"lineNumber": 53, "col_offset": 30, "nodeName": "text", "type": "Any"}, {"lineNumber": 12, "col_offset": 34, "nodeName": "nlp", "type": "Any"}, {"lineNumber": 12, "col_offset": 21, "nodeName": "nlp", "type": "Any"}, {"lineNumber": 12, "col_offset": 55, "nodeName": "create_vocab", "type": "Any"}, {"lineNumber": 12, "col_offset": 72, "nodeName": "nlp", "type": "Any"}, {"lineNumber": 21, "col_offset": 17, "nodeName": "x", "type": "Any"}, {"lineNumber": 21, "col_offset": 31, "nodeName": "x", "type": "Any"}, {"lineNumber": 22, "col_offset": 19, "nodeName": "self", "type": "JapaneseTokenizer"}, {"lineNumber": 22, "col_offset": 37, "nodeName": "words", "type": "List[Any]"}, {"lineNumber": 40, "col_offset": 28, "nodeName": "Language", "type": "Any"}, {"lineNumber": 53, "col_offset": 15, "nodeName": "self", "type": "Japanese"}, {"lineNumber": 12, "col_offset": 55, "nodeName": "cls", "type": "Type[JapaneseDefaults]"}, {"lineNumber": 16, "col_offset": 18, "nodeName": "ImportError", "type": "Type[ImportError]"}, {"lineNumber": 21, "col_offset": 36, "nodeName": "tokenize", "type": "Any"}, {"lineNumber": 21, "col_offset": 60, "nodeName": "text", "type": "Any"}, {"lineNumber": 21, "col_offset": 36, "nodeName": "tokenizer", "type": "Any"}, {"lineNumber": 22, "col_offset": 59, "nodeName": "len", "type": "Callable[[Sized], int]"}, {"lineNumber": 22, "col_offset": 63, "nodeName": "words", "type": "List[Any]"}, {"lineNumber": 21, "col_offset": 36, "nodeName": "self", "type": "JapaneseTokenizer"}]
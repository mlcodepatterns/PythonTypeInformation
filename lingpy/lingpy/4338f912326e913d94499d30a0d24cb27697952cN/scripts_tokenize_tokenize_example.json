[{"lineNumber": 6, "col_offset": 0, "nodeName": "__author__", "type": "str"}, {"lineNumber": 7, "col_offset": 0, "nodeName": "__date__", "type": "str"}, {"lineNumber": 11, "col_offset": 0, "nodeName": "word", "type": "str"}, {"lineNumber": 13, "col_offset": 0, "nodeName": "paths", "type": "List[str]"}, {"lineNumber": 15, "col_offset": 4, "nodeName": "path", "type": "str"}, {"lineNumber": 15, "col_offset": 12, "nodeName": "paths", "type": "List[str]"}, {"lineNumber": 33, "col_offset": 0, "nodeName": "t", "type": "lingpy.sequence.tokenizer.Tokenizer"}, {"lineNumber": 16, "col_offset": 4, "nodeName": "t", "type": "lingpy.sequence.tokenizer.Tokenizer"}, {"lineNumber": 33, "col_offset": 4, "nodeName": "Tokenizer", "type": "Type[lingpy.sequence.tokenizer.Tokenizer]"}, {"lineNumber": 16, "col_offset": 8, "nodeName": "Tokenizer", "type": "Type[lingpy.sequence.tokenizer.Tokenizer]"}, {"lineNumber": 16, "col_offset": 18, "nodeName": "path", "type": "str"}, {"lineNumber": 36, "col_offset": 6, "nodeName": "word", "type": "str"}, {"lineNumber": 18, "col_offset": 31, "nodeName": "path", "type": "str"}, {"lineNumber": 20, "col_offset": 10, "nodeName": "word", "type": "str"}, {"lineNumber": 37, "col_offset": 6, "nodeName": "characters", "type": "Callable[[protocols.SupportsReplace], Union[str, unicode]]"}, {"lineNumber": 37, "col_offset": 19, "nodeName": "word", "type": "str"}, {"lineNumber": 38, "col_offset": 6, "nodeName": "grapheme_clusters", "type": "Callable[[protocols.SupportsReplace], Union[str, unicode, protocols.SupportsSplit]]"}, {"lineNumber": 38, "col_offset": 26, "nodeName": "word", "type": "str"}, {"lineNumber": 21, "col_offset": 10, "nodeName": "characters", "type": "Callable[[protocols.SupportsReplace], Union[str, unicode]]"}, {"lineNumber": 21, "col_offset": 23, "nodeName": "word", "type": "str"}, {"lineNumber": 22, "col_offset": 10, "nodeName": "grapheme_clusters", "type": "Callable[[protocols.SupportsReplace], Union[str, unicode, protocols.SupportsSplit]]"}, {"lineNumber": 22, "col_offset": 30, "nodeName": "word", "type": "str"}, {"lineNumber": 23, "col_offset": 10, "nodeName": "graphemes", "type": "Callable[[Union[str, unicode]], Union[str, unicode, protocols.SupportsSplit]]"}, {"lineNumber": 23, "col_offset": 22, "nodeName": "word", "type": "str"}, {"lineNumber": 24, "col_offset": 10, "nodeName": "transform", "type": "Callable[..., Any]"}, {"lineNumber": 24, "col_offset": 22, "nodeName": "word", "type": "str"}, {"lineNumber": 25, "col_offset": 10, "nodeName": "transform", "type": "Callable[..., Any]"}, {"lineNumber": 25, "col_offset": 22, "nodeName": "word", "type": "str"}, {"lineNumber": 26, "col_offset": 10, "nodeName": "transform", "type": "Callable[..., Any]"}, {"lineNumber": 26, "col_offset": 22, "nodeName": "word", "type": "str"}, {"lineNumber": 27, "col_offset": 10, "nodeName": "rules", "type": "Callable"}, {"lineNumber": 27, "col_offset": 18, "nodeName": "word", "type": "str"}, {"lineNumber": 28, "col_offset": 10, "nodeName": "transform_rules", "type": "Callable[[Any], Any]"}, {"lineNumber": 28, "col_offset": 28, "nodeName": "word", "type": "str"}, {"lineNumber": 37, "col_offset": 6, "nodeName": "t", "type": "lingpy.sequence.tokenizer.Tokenizer"}, {"lineNumber": 38, "col_offset": 6, "nodeName": "t", "type": "lingpy.sequence.tokenizer.Tokenizer"}, {"lineNumber": 21, "col_offset": 10, "nodeName": "t", "type": "lingpy.sequence.tokenizer.Tokenizer"}, {"lineNumber": 22, "col_offset": 10, "nodeName": "t", "type": "lingpy.sequence.tokenizer.Tokenizer"}, {"lineNumber": 23, "col_offset": 10, "nodeName": "t", "type": "lingpy.sequence.tokenizer.Tokenizer"}, {"lineNumber": 24, "col_offset": 10, "nodeName": "t", "type": "lingpy.sequence.tokenizer.Tokenizer"}, {"lineNumber": 25, "col_offset": 10, "nodeName": "t", "type": "lingpy.sequence.tokenizer.Tokenizer"}, {"lineNumber": 26, "col_offset": 10, "nodeName": "t", "type": "lingpy.sequence.tokenizer.Tokenizer"}, {"lineNumber": 27, "col_offset": 10, "nodeName": "t", "type": "lingpy.sequence.tokenizer.Tokenizer"}, {"lineNumber": 28, "col_offset": 10, "nodeName": "t", "type": "lingpy.sequence.tokenizer.Tokenizer"}]
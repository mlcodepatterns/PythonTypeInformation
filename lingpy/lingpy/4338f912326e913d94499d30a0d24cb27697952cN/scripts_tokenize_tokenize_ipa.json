[{"lineNumber": 5, "col_offset": 0, "nodeName": "__author__", "type": "str"}, {"lineNumber": 6, "col_offset": 0, "nodeName": "__date__", "type": "str"}, {"lineNumber": 12, "col_offset": 0, "nodeName": "t", "type": "lingpy.sequence.tokenizer.Tokenizer"}, {"lineNumber": 12, "col_offset": 4, "nodeName": "Tokenizer", "type": "Type[lingpy.sequence.tokenizer.Tokenizer]"}, {"lineNumber": 13, "col_offset": 0, "nodeName": "tokenize_ipa", "type": "Callable[[Any], Any]"}, {"lineNumber": 15, "col_offset": 66, "nodeName": "file", "type": "Any"}, {"lineNumber": 16, "col_offset": 8, "nodeName": "line", "type": "Any"}, {"lineNumber": 16, "col_offset": 16, "nodeName": "file", "type": "Any"}, {"lineNumber": 13, "col_offset": 0, "nodeName": "t", "type": "lingpy.sequence.tokenizer.Tokenizer"}, {"lineNumber": 15, "col_offset": 5, "nodeName": "open", "type": "Any"}, {"lineNumber": 17, "col_offset": 8, "nodeName": "line", "type": "Any"}, {"lineNumber": 18, "col_offset": 8, "nodeName": "tokens", "type": "Any"}, {"lineNumber": 19, "col_offset": 12, "nodeName": "token", "type": "Any"}, {"lineNumber": 19, "col_offset": 21, "nodeName": "tokens", "type": "Any"}, {"lineNumber": 15, "col_offset": 5, "nodeName": "codecs", "type": "module"}, {"lineNumber": 17, "col_offset": 15, "nodeName": "strip", "type": "Any"}, {"lineNumber": 18, "col_offset": 17, "nodeName": "split", "type": "Any"}, {"lineNumber": 20, "col_offset": 12, "nodeName": "token", "type": "Any"}, {"lineNumber": 17, "col_offset": 15, "nodeName": "line", "type": "Any"}, {"lineNumber": 18, "col_offset": 17, "nodeName": "line", "type": "Any"}, {"lineNumber": 20, "col_offset": 20, "nodeName": "strip", "type": "Any"}, {"lineNumber": 21, "col_offset": 12, "nodeName": "print", "type": "Callable[..., None]"}, {"lineNumber": 20, "col_offset": 20, "nodeName": "token", "type": "Any"}, {"lineNumber": 21, "col_offset": 18, "nodeName": "token", "type": "Any"}, {"lineNumber": 21, "col_offset": 29, "nodeName": "tokenize_ipa", "type": "Callable[[Any], Any]"}, {"lineNumber": 21, "col_offset": 44, "nodeName": "token", "type": "Any"}, {"lineNumber": 21, "col_offset": 29, "nodeName": "t", "type": "lingpy.sequence.tokenizer.Tokenizer"}]
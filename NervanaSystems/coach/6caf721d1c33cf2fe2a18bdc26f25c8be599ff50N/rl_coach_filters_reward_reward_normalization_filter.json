[{"lineNumber": 27, "col_offset": 32, "nodeName": "RewardFilter", "type": "Type[rl_coach.filters.reward.reward_filter.RewardFilter]"}, {"lineNumber": 65, "col_offset": 78, "nodeName": "RewardType", "type": "Union[Type[float], Type[int], Type[numpy.ndarray]]"}, {"lineNumber": 75, "col_offset": 76, "nodeName": "RewardSpace", "type": "Type[rl_coach.spaces.RewardSpace]"}, {"lineNumber": 39, "col_offset": 8, "nodeName": "clip_min", "type": "float"}, {"lineNumber": 39, "col_offset": 24, "nodeName": "clip_min", "type": "float"}, {"lineNumber": 40, "col_offset": 8, "nodeName": "clip_max", "type": "float"}, {"lineNumber": 40, "col_offset": 24, "nodeName": "clip_max", "type": "float"}, {"lineNumber": 41, "col_offset": 8, "nodeName": "running_rewards_stats", "type": "None"}, {"lineNumber": 66, "col_offset": 11, "nodeName": "update_internal_state", "type": "bool"}, {"lineNumber": 71, "col_offset": 8, "nodeName": "reward", "type": "Any"}, {"lineNumber": 73, "col_offset": 15, "nodeName": "reward", "type": "Any"}, {"lineNumber": 76, "col_offset": 15, "nodeName": "input_reward_space", "type": "rl_coach.spaces.RewardSpace"}, {"lineNumber": 33, "col_offset": 33, "nodeName": "float", "type": "Type[float]"}, {"lineNumber": 33, "col_offset": 55, "nodeName": "float", "type": "Type[float]"}, {"lineNumber": 38, "col_offset": 8, "nodeName": "__init__", "type": "Callable[[], None]"}, {"lineNumber": 39, "col_offset": 8, "nodeName": "self", "type": "RewardNormalizationFilter"}, {"lineNumber": 40, "col_offset": 8, "nodeName": "self", "type": "RewardNormalizationFilter"}, {"lineNumber": 41, "col_offset": 8, "nodeName": "self", "type": "RewardNormalizationFilter"}, {"lineNumber": 50, "col_offset": 11, "nodeName": "mode", "type": "Any"}, {"lineNumber": 63, "col_offset": 8, "nodeName": "set_session", "type": "Any"}, {"lineNumber": 63, "col_offset": 47, "nodeName": "sess", "type": "Any"}, {"lineNumber": 65, "col_offset": 29, "nodeName": "RewardType", "type": "Union[Type[float], Type[int], Type[numpy.ndarray]]"}, {"lineNumber": 65, "col_offset": 64, "nodeName": "bool", "type": "Type[bool]"}, {"lineNumber": 71, "col_offset": 17, "nodeName": "clip", "type": "Any"}, {"lineNumber": 71, "col_offset": 25, "nodeName": "reward", "type": "Any"}, {"lineNumber": 71, "col_offset": 33, "nodeName": "clip_min", "type": "float"}, {"lineNumber": 71, "col_offset": 48, "nodeName": "clip_max", "type": "float"}, {"lineNumber": 75, "col_offset": 60, "nodeName": "RewardSpace", "type": "Type[rl_coach.spaces.RewardSpace]"}, {"lineNumber": 51, "col_offset": 41, "nodeName": "TFSharedRunningStats", "type": "Type[rl_coach.architectures.tensorflow_components.shared_variables.TFSharedRunningStats]"}, {"lineNumber": 51, "col_offset": 62, "nodeName": "device", "type": "Any"}, {"lineNumber": 53, "col_offset": 13, "nodeName": "mode", "type": "Any"}, {"lineNumber": 63, "col_offset": 8, "nodeName": "running_rewards_stats", "type": "Any"}, {"lineNumber": 67, "col_offset": 12, "nodeName": "push", "type": "Any"}, {"lineNumber": 67, "col_offset": 44, "nodeName": "reward", "type": "Union[int, float, numpy.ndarray]"}, {"lineNumber": 69, "col_offset": 18, "nodeName": "reward", "type": "Union[int, float, numpy.ndarray]"}, {"lineNumber": 69, "col_offset": 27, "nodeName": "mean", "type": "Any"}, {"lineNumber": 70, "col_offset": 23, "nodeName": "std", "type": "Any"}, {"lineNumber": 71, "col_offset": 17, "nodeName": "np", "type": "module"}, {"lineNumber": 71, "col_offset": 33, "nodeName": "self", "type": "RewardNormalizationFilter"}, {"lineNumber": 71, "col_offset": 48, "nodeName": "self", "type": "RewardNormalizationFilter"}, {"lineNumber": 38, "col_offset": 8, "nodeName": "super", "type": "Type[super]"}, {"lineNumber": 52, "col_offset": 74, "nodeName": "memory_backend_params", "type": "Any"}, {"lineNumber": 54, "col_offset": 41, "nodeName": "NumpySharedRunningStats", "type": "Type[rl_coach.utilities.shared_running_stats.NumpySharedRunningStats]"}, {"lineNumber": 63, "col_offset": 8, "nodeName": "self", "type": "RewardNormalizationFilter"}, {"lineNumber": 67, "col_offset": 12, "nodeName": "running_rewards_stats", "type": "Any"}, {"lineNumber": 69, "col_offset": 27, "nodeName": "running_rewards_stats", "type": "Any"}, {"lineNumber": 70, "col_offset": 23, "nodeName": "running_rewards_stats", "type": "Any"}, {"lineNumber": 55, "col_offset": 72, "nodeName": "memory_backend_params", "type": "Any"}, {"lineNumber": 67, "col_offset": 12, "nodeName": "self", "type": "RewardNormalizationFilter"}, {"lineNumber": 69, "col_offset": 27, "nodeName": "self", "type": "RewardNormalizationFilter"}, {"lineNumber": 70, "col_offset": 23, "nodeName": "self", "type": "RewardNormalizationFilter"}]
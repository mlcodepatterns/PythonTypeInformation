[{"lineNumber": 6, "col_offset": 29, "nodeName": "AllenNlpTestCase", "type": "Type[allennlp.common.testing.test_case.AllenNlpTestCase]"}, {"lineNumber": 8, "col_offset": 8, "nodeName": "tokenizer", "type": "Any"}, {"lineNumber": 9, "col_offset": 8, "nodeName": "sentence", "type": "str"}, {"lineNumber": 10, "col_offset": 8, "nodeName": "tokens", "type": "List[Any]"}, {"lineNumber": 16, "col_offset": 8, "nodeName": "tokenizer", "type": "Any"}, {"lineNumber": 17, "col_offset": 8, "nodeName": "word", "type": "str"}, {"lineNumber": 18, "col_offset": 8, "nodeName": "tokens", "type": "List[Any]"}, {"lineNumber": 20, "col_offset": 8, "nodeName": "expected_tokens", "type": "List[int]"}, {"lineNumber": 8, "col_offset": 20, "nodeName": "CharacterTokenizer", "type": "Any"}, {"lineNumber": 10, "col_offset": 18, "nodeName": "text", "type": "Any"}, {"lineNumber": 13, "col_offset": 15, "nodeName": "tokens", "type": "List[Any]"}, {"lineNumber": 13, "col_offset": 25, "nodeName": "expected_tokens", "type": "List[str]"}, {"lineNumber": 16, "col_offset": 20, "nodeName": "CharacterTokenizer", "type": "Any"}, {"lineNumber": 18, "col_offset": 18, "nodeName": "text_id", "type": "Any"}, {"lineNumber": 21, "col_offset": 15, "nodeName": "tokens", "type": "List[Any]"}, {"lineNumber": 21, "col_offset": 25, "nodeName": "expected_tokens", "type": "List[int]"}, {"lineNumber": 10, "col_offset": 18, "nodeName": "t", "type": "Any"}, {"lineNumber": 10, "col_offset": 29, "nodeName": "t", "type": "Any"}, {"lineNumber": 18, "col_offset": 18, "nodeName": "t", "type": "Any"}, {"lineNumber": 18, "col_offset": 32, "nodeName": "t", "type": "Any"}, {"lineNumber": 10, "col_offset": 34, "nodeName": "tokenize", "type": "Any"}, {"lineNumber": 10, "col_offset": 53, "nodeName": "sentence", "type": "str"}, {"lineNumber": 18, "col_offset": 37, "nodeName": "tokenize", "type": "Any"}, {"lineNumber": 18, "col_offset": 56, "nodeName": "word", "type": "str"}, {"lineNumber": 10, "col_offset": 34, "nodeName": "tokenizer", "type": "Any"}, {"lineNumber": 18, "col_offset": 37, "nodeName": "tokenizer", "type": "Any"}]
[{"lineNumber": 10, "col_offset": 0, "nodeName": "tokenizers", "type": "Dict[str, Union[Type[allennlp.data.tokenizers.tokenizer.Tokenizer], Type[allennlp.data.tokenizers.word_tokenizer.WordTokenizer], Type[allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer]]]"}, {"lineNumber": 13, "col_offset": 22, "nodeName": "WordTokenizer", "type": "Type[allennlp.data.tokenizers.word_tokenizer.WordTokenizer]"}, {"lineNumber": 14, "col_offset": 27, "nodeName": "CharacterTokenizer", "type": "Type[allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer]"}, {"lineNumber": 10, "col_offset": 13, "nodeName": "OrderedDict", "type": "Type[collections.OrderedDict[Any, Any]]"}, {"lineNumber": 13, "col_offset": 0, "nodeName": "tokenizers", "type": "Dict[str, Union[Type[allennlp.data.tokenizers.tokenizer.Tokenizer], Type[allennlp.data.tokenizers.word_tokenizer.WordTokenizer], Type[allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer]]]"}, {"lineNumber": 14, "col_offset": 0, "nodeName": "tokenizers", "type": "Dict[str, Union[Type[allennlp.data.tokenizers.tokenizer.Tokenizer], Type[allennlp.data.tokenizers.word_tokenizer.WordTokenizer], Type[allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer]]]"}]